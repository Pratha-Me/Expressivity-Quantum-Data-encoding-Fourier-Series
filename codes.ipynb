{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Replication of FIG. 3.\n",
    "\n",
    "From the article, the FIG. 3, is extracted and placed below.\n",
    "\n",
    "<figure>\n",
    "<img src=\"assets/images/FIG3.png\" alt=\"Training of different models with different scales\" width=\"325\"/>\n",
    "<figcaption><b>Figure 1 - Quantum Data encoding model for a multi-qubit system</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "The figure caption is quoted below.\n",
    "\n",
    ">FIG. 3. A parametrised quantum model is trained with data samples (white circles) to ﬁt a target function g(x) = ′ ∑ n=−1 1 c n e −nix or g (x) = ∑ n=−2 2 c n e −nix with coeﬃcients c 0 = 0.1, c 1 = c 2 = 0.15 − 0.15i. The variational circuit is of the form f(x) = 〈 0 | U † (x)σ z U(x) | 0 〉 where | 0 〉 is a single qubit, and U = W (2) R x (x)W (1) . The W (round blue symbols) are implemented as general rotation gates parametrised by three learnable weights each, and Rx  (square blue symbols) is a single Pauli-X rotation. The left panels show the quantum model function f(x) and target function g(x), g ′ (x), while the right panels show the mean squared error between the data sampled from g and f during a typical training run. Feeding in the input x as is (top row), the quantum model easily ﬁts the target of degree 1. Rescaling the inputs x → 2x causes a frequency mismatch, and the model cannot learn the target any more (middle row). However, even with the correct scaling, the variational circuit cannot ﬁt the target function of degree 2 (bottom row). The experiments in this paper were all performed using the PennyLane software library [36]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODE: Target Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def square_loss(targets, predictions):\n",
    "    loss = 0\n",
    "    for t, p in zip(targets, predictions):\n",
    "        loss += (t - p) ** 2\n",
    "    loss = loss / len(targets)\n",
    "    return 0.5*loss\n",
    "\n",
    "degree = 1  # degree of the target function\n",
    "coeffs = [0.15 + 0.15j]*degree  # coefficients of non-zero frequencies\n",
    "coeff_0 = 0.1  # coefficient of zero frequency\n",
    "data_points = 100  # number of datas\n",
    "scale_target = 1.  # scale_target of the data\n",
    "\n",
    "def target_function(x):\n",
    "    res = 0.0 + 0.0j\n",
    "    for idx, coeff in enumerate(coeffs):\n",
    "        exponent = np.complex128((idx+1) * 1j * scale_target * x)\n",
    "        conj_coeff = np.conjugate(coeff)\n",
    "        res += coeff * np.exp(exponent) + conj_coeff * np.exp(-exponent)\n",
    "    return np.real(res + coeff_0)\n",
    "\n",
    "x = np.linspace(-6, 6, data_points, requires_grad=False)\n",
    "target_y = np.array([target_function(x_) for x_ in x], requires_grad=False)\n",
    "\n",
    "plt.plot(x, target_y, color='black')\n",
    "plt.scatter(x, target_y, facecolor='white', edgecolor='black')\n",
    "plt.ylim(-1, 1)\n",
    "plt.axvline(0.0)\n",
    "plt.axhline(0.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first code block went smooth! The import and variable declaration is fine and all, yet there might be a small detail asking for a sharp attention. It's the fifth variable ```scale_target``` declared to $1$. If this is anything but $1$ then the loss is huge and the trainable model all of the sudden becomes untrainable. This is what authors precisely meant is the account between the expressivity and the data encoding strategy. \n",
    "\n",
    "Better yet, define a ```scale_train``` and set it to $1$. Now as long as the difference between ```scale_target``` and ```scale_train_model``` remains $0$ the model is trainable otherwise if not.\n",
    "\n",
    "The second row of the FIG. 3. is the ouput for  ```scale_target = 1``` and ```scale_train_model = 2```. I have coded for such case in following section 1.2.\n",
    "\n",
    "Finally, let's make a trainable model to train it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainable model randomly instantiated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_train_model = 1\n",
    "dev = qml.device('default.qubit', wires=1)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def quantum_model(weights, x):\n",
    "\n",
    "    for (idx, theta) in enumerate(weights):\n",
    "\n",
    "        ''' This is trainable circuit block. theta_tensor is the tensor containing the weights. The value of weights adjusts to approach the target function as close as possible. The updates happen batchwise. \n",
    "        \n",
    "        The Rot method in qml is the general rotation operator which takes in three parameters theta, phi and a phase. '''\n",
    "        qml.Rot(theta[0], theta[1], theta[2], wires=0)\n",
    "\n",
    "        ''' This is encoding gate. It's a roation gate, if x = pi then this is a pauli X-gate '''\n",
    "        if (idx == len(weights) -1):\n",
    "            continue\n",
    "        \n",
    "        qml.RX(scale_train_model*x, wires=0)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(wires=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'weights' is a tensor. In this single-qubit case, it a $1\\times3$ row matrix. In a n-qubit model it's $n\\times3$ matrix. The three sticks around because 'Rot' method in the class 'qml' takes in exact three parameters. Moreover, the quantum model returns an expectation value for the Pauli Z-gate. Since Hadamard gate is never applied throught the entire model the Pauli Z-gate has no effect like that of an Identity operator/matrix. Well almost! Except for an additional phase of $\\pi$ when operated on the state/qubit $|1>. \n",
    "\n",
    "$$\\sigma_z|1> = e^{i\\pi}|1>$$ \n",
    "\n",
    "Yet, this difference won't impact the measurement since the phase term is cancelled out by its conjugate during the measurement operation. So, for all the intend of measurement Pauli Z-gate has no effect! As simple as:\n",
    "\n",
    "$$\n",
    "<1|\\sigma_z^{\\dagger}\\sigma_z|1> = <1|e^{-i\\pi}e^{i\\pi}|1>\n",
    "$$\n",
    "$$\n",
    "\\therefore <1|\\sigma_z^{\\dagger}\\sigma_z|1> = <1|1> \n",
    "$$\n",
    "\n",
    "I must mention there're nice places to play with qubits [8] [9]. The websites provide a visual and dataful experience. \n",
    "\n",
    "In the next code snippet, I have built a random trainable model. After it's visualized it's time to begin the long awaited training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times the encoding gets repeated (here equal to the number of layers)\n",
    "r = 1\n",
    "\n",
    "# some random initial weights\n",
    "weights = 2 * np.pi * np.random.random(size=(r+1, 3), requires_grad=True)\n",
    "\n",
    "x = np.linspace(-6, 6, data_points, requires_grad=False)\n",
    "random_quantum_model_y = [quantum_model(weights, x_) for x_ in x]\n",
    "\n",
    "plt.plot(x, random_quantum_model_y, color='blue')\n",
    "plt.ylim(-1.25, 1.25)\n",
    "plt.axvline(0.0)\n",
    "plt.axhline(0.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qml.draw(quantum_model)(weights, x[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The first row\n",
    "To obtain this result, the scale for target and training model must be $1$. In the code snippet below they are the variables ```scale_target = 1``` and ```scale_train_model = 1```.\n",
    "\n",
    "The degree of truncated Fourier series in $1$. There's one qubit in the model, so, the number of encoding gate is $1$ too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Optimization/Learning for the parameteric circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(weights, x, y):\n",
    "    predictions = [quantum_model(weights, x_) for x_ in x]\n",
    "    return square_loss(y, predictions)\n",
    "\n",
    "cost_ = [cost(weights, x, target_y)]\n",
    "\n",
    "def optimizer_func(weights):\n",
    "    # max_steps = 150\n",
    "    # opt = qml.AdamOptimizer(stepsize=0.25)\n",
    "    # batch_size = 30\n",
    "\n",
    "    max_steps = 120\n",
    "    opt = qml.AdamOptimizer(stepsize=0.4)\n",
    "    batch_size = 40\n",
    "\n",
    "\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        batch_index = np.random.randint(0, len(x), (batch_size,))\n",
    "        x_batch = x[batch_index]\n",
    "        y_batch = target_y[batch_index]\n",
    "\n",
    "        # Update the weights by one optimizer step\n",
    "        weights, _, _ = opt.step(cost, weights, x_batch, y_batch)\n",
    "\n",
    "        # Save, and possibly print, the current cost\n",
    "        c = cost(weights, x, target_y)\n",
    "        cost_.append(c)\n",
    "        \n",
    "        if (step + 1) % 15 == 0:\n",
    "            print(\"Cost at step {0:3}: {1}\".format(step + 1, c))\n",
    "    \n",
    "    return (weights, cost_)\n",
    "\n",
    "(weights_scale_1_1, cost_1_1 )= optimizer_func(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Result\n",
    "The Loss profile for the training and the graph with both traget model and trained model plotted together is placed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(cost_1_1)), cost_1_1)\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylim(0, 0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [quantum_model(weights_scale_1_1, x_) for x_ in x]\n",
    "\n",
    "plt.plot(x, target_y, c='black')\n",
    "plt.scatter(x, target_y, facecolor='white', edgecolor='black')\n",
    "plt.plot(x, predictions, c='red')\n",
    "plt.ylim(-1,1)\n",
    "plt.axvline(0.0)\n",
    "plt.axhline(0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Conclusion\n",
    "When the variables ```scale_target = 1``` and ```scale_train_model = 1```, in other words, the scale is identica then the parametric variational model learns with very minimum loss. The quantum model is trainable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The second row - Change of scale! \n",
    "The above result was for scale_target = 1. and scale_train_model = 1. To obtain the second row figure from the FIG. 3 from the paper set the scale_train_model = 2\n",
    "\n",
    "Finally, trigger the optimizer!\n",
    "\n",
    "### 1.2.1 Optimization/Learning for the parameteric circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_train_model = 2\n",
    "\n",
    "# Reinitialize the (seeded) random initial weights\n",
    "weights = 2 * np.pi * np.random.random(size=(r+1, 3), requires_grad=True)\n",
    "cost_ = [cost(weights, x, target_y)]\n",
    "\n",
    "#  Run the optimizer for scale_target = 1 and scale_train_model = 2\n",
    "(weights_scale_1_2, cost_1_2)= optimizer_func(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(cost_1_2)), cost_1_2)\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylim(0, 0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [quantum_model(weights_scale_1_2, x_) for x_ in x]\n",
    "\n",
    "plt.plot(x, target_y, c='black')\n",
    "plt.scatter(x, target_y, facecolor='white', edgecolor='black')\n",
    "plt.plot(x, predictions, c='red')\n",
    "plt.ylim(-1,1)\n",
    "plt.axvline(0.0)\n",
    "plt.axhline(0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Conclusion\n",
    "When the variables ```scale_target = 1``` and ```scale_train_model = 2```, in other words, the scale is different then the parametric variational model doesn't learn. The loss does not minimise. The quantum model is untrainable!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e7fa02fd4aaacf9a09f07b3e15c3219fdcf8d139f335f7e481029fdf2680c63"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('quantum')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
